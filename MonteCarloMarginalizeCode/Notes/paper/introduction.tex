\label{sec:introduction}

Ground based gravitational wave detector networks (notably LIGO \cite{gw-detectors-LIGO-original-preferred} and Virgo
\cite{gw-detectors-VIRGO-original-preferred})  are sensitive to the gravitational wave signal from coalescing compact
binaries, both the relatively well understood signal from  the lowest-mass compact binaries
$M=m_1+m_2\le 16 M_\odot$
\cite{2003PhRvD..67j4025B,2004PhRvD..70j4003B,2004PhRvD..70f4028D,BCV:PTF,2005PhRvD..71b4039K,2005PhRvD..72h4027B,2006PhRvD..73l4012K,2007MNRAS.374..721T,2008PhRvD..78j4007H,gr-astro-eccentric-NR-2008,gw-astro-mergers-approximations-SpinningPNHigherHarmonics,gw-astro-PN-Comparison-AlessandraSathya2009}
and the less-well-understood strong-field epoch \editremark{citations}.    
%
% POINT: Inference via Bayesian methods
Interpreting gravitational wave data requires systematically comparing all possible candidate signals to the data,
constructing a Bayesian posterior probability distribution for candidate binary parameters \citeMCMC{}.   
%
Owing to the complexity and multimodality of the model space, historically successful strategies have adopted  generic, serial
algorithms for parameter estimation, such as variants of Markov Chain Monte Carlo or nested sampling
\cite{2011RvMP...83..943V,gw-astro-PE-lalinference-v1}.  
% POINT: Why do something new?  
%   -Because they are slow, being serial
Though successful, these generic algorithms are functionally serial, requiring intensive communication to coordinate the
current state, and are therefore well-known to scale poorly to large numbers of processors.  
%
These algorithms' convergence is also limited by \emph{ergodicity}.  No theorem guarantees these algorithm \emph{must}
 explore the entire model space, let alone efficiently; no expression can robustly assess convergence, using available
 sampled data.  
%
By contrast, well-understood, efficient, and highly-parallelizable Monte Carlo integration strategies are frequently applied to problems with dimensions comparable or
higher than coalescing compact binaries \textbf{citations}.    In this work, we apply such methods to gravitational wave
parameter estimation for the first time.  
%


% POINT :
%  - because they are slow, not communicating 
Gravitational wave parameter estimation has also been limited by the computational cost associated with comparing the
data with a candidate waveform.  This cost has two factors: first, the cost of waveform generation, as discussed in
\cite{gwastro-mergers-PE-ReducedOrder-2013,2013PhRvD..87l2002S,2013PhRvD..87d4008C,gwastro-mergers-IMRPhenomP,gwastro-SpinTaylorF2-2013} and references therein; and, second,
the cost of repeatedly operating on the long time- or frequency-series itself, once per comparison (e.g., fourier
transforms).   
%
Recently, several methods have been proposed to perform this comparison more efficiently
\cite{gwastro-mergers-PE-ReducedOrder-2013,2013PhRvD..87l2002S,2013PhRvD..87d4008C,gw-astro-ReducedOrderQuadraturePE-TiglioEtAl2014}, by interpolating some combination
of the the waveform or likelihood or by adopting a sparse representation to reduce the computational cost of data
handling.  
%
In this work, we introduce a new, simple, and extremely robust scheme to reduce the cost per comparison.  
%
To our knowledge, this work is the first time any such method has been implemented at production scale, particularly for
the most accurate and computationally-expensive waveform models like EOB
\cite{gw-astro-EOBspin-Tarrachini2012,gw-astro-EOBNR-Calibrated-2009}.  


% POINT: Blindly redo the anaysis, versus using information from search?
Historically, parameter estimation strategies very practically make almost no use of the information reported by
existing search codes.  This careful approach ensured, for example, that inferences from the data would never be used as priors on a
reanalysis of the same data.  
%
That said, experience suggests the search codes provide a reasonable first approximation, 
particularly for the most expensive parameters (i.e., event time; chirp time; sky location).  
%
Some parameter estimation codes have used this information to improve their performance, more efficiently searching the
sky \editremark{check with Ben F}.  
That said, historical experience with serial codes suggest \emph{sampling} the maximum (not finding it) is what limits
performance; based on that experience, one might expect minimal additional asymptotic speed improvement when provided
with a good initial guess.
% ``burnin is short; finding the best fit is fast;
%what's expensive is mapping out the likelihood near the maximum'' \editremark{fixme: can't use in publication}.
%
As our algorithm operates in a different regime,  we describe a procedure systematically and organically incorporates information provided
by the search to accelerate our specific algorithm's performance.  


% POINT: Outline
This work is organized as follows.
In Section \ref{sec:Executive} we provide an executive summary, providing the principles that enable our algorithm to
provide rapid, accurate results for the test cases explored here:  time-domain models for nonprecessing binaries.  
%
Then, in Section \ref{sec:Methods}, we describe our algorithm in detail.  
%
To demonstrate  our algorithm provides high performance in a production environment,  Section \ref{sec:Results} we
provide concrete results drawn from a large sample of events from the ``2015 double neutron star mock data challenge'',
results from which will be described in \editremark{first2years, others}.  
%
In Section \ref{sec:Discussion} we reflect on the broader significance of our result, in the context of other parameter
estimation work inside and outside the LSC.  
%
We conclude with Section \ref{sec:Conclude}
