#!/usr/bin/env python

"""
Compute the likelihood of parameters of a GW signal given some data
that has been marginalized over extrinsic parameters. Creates a dag workflow
to perform this calculation.
"""

import os
import sys
import stat
import glue.lal
from glue.ligolw import utils, ligolw, lsctables, table
from glue.ligolw.utils import process
import lal
import lalsimulation as lalsim
import lalsimutils as lsu
import effectiveFisher as eff
import dag_utils
import numpy as np
from functools import partial
from optparse import OptionParser, OptionGroup

__author__ = "Evan Ochsner <evano@gravity.phys.uwm.edu>, Chris Pankow <pankow@gravity.phys.uwm.edu>, R. O'Shaughnessy"

def mkdir(dir_name):
    try :
        os.mkdir(dir_name)
    except OSError:
        pass

#
# Pinnable parameters -- for command line processing
#
LIKELIHOOD_PINNABLE_PARAMS = ["right ascension", "declination"]

#
# Option parsing
#

optp = OptionParser()
# Options needed by this program only.
optp.add_option("-X", "--mass-points-xml", action="store_true", help="Output mass points as a sim_inspiral table.")
optp.add_option("-N", "--N-mass-pts", type=int, default=200, help="Number of intrinsic parameter (mass) values at which to compute marginalized likelihood. Default is 200.")

# Options transferred to ILE
optp.add_option("-c", "--cache-file", default=None, help="LIGO cache file containing all data needed.")
optp.add_option("-C", "--channel-name", action="append", help="instrument=channel-name, e.g. H1=FAKE-STRAIN. Can be given multiple times for different instruments.")
optp.add_option("-p", "--psd-file", action="append", help="instrument=psd-file, e.g. H1=H1_PSD.xml.gz. Can be given multiple times for different instruments.")
optp.add_option("-x", "--coinc-xml", help="gstlal_inspiral XML file containing coincidence information.")
optp.add_option("-f", "--reference-freq", type=float, default=100.0, help="Waveform reference frequency. Required, default is 100 Hz.")
optp.add_option("-F", "--fmax", type=float, help="Upper frequency of signal integration. Default is use PSD's maximum frequency.")
optp.add_option("-e", "--event-time", type=float, help="GPS time of the event --- probably the end time. Required if --coinc-xml not given.")
optp.add_option("-m", "--time-marginalization", action="store_true", help="Perform marginalization over time via direct numerical integration. Default is false.")
optp.add_option("-S", "--save-samples", action="store_true", help="Save sample points to output-file. Requires --output-file to be defined.")
optp.add_option("-o", "--output-file", help="Save result to this file.")
optp.add_option("-n", "--n-copies", type=int, default=1, help="Number of copies to do of a given run. Default is 1.")
optp.add_option("--n-max", type=int, default=1000000, help="Maximum number of throws to use in integrator.")
optp.add_option("--n-eff", type=int, default=1000, help="Maximum number of effective samples to reach before terminating.")
optp.add_option("-L", "--save-deltalnL", type=float, default=float("Inf"), help="Threshold on deltalnL for points preserved in output file.  Requires --output-file to be defined")
optp.add_option("-P", "--save-P", type=float,default=0, help="Threshold on cumulative probability for points preserved in output file.  Requires --output-file to be defined")
optp.add_option("--n-chunk", type=int, help="Chunk'.",default=100)
optp.add_option("--adapt-floor-level", type=float,default=0.0,help="Floor to use with weights (likelihood integrand) when doing adaptive sampling. This is necessary to ensure the *sampling* prior is non zero during adaptive sampling and to prevent overconvergence. Default is 0.0 (no floor)")
optp.add_option("--adapt-weight-exponent", type=float,default=1.0, help="Exponent to use with weights (likelihood integrand) when doing adaptive sampling. Used in tandem with --adapt-floor-level to prevent overconvergence. Default is 1.0.")

#
# Add the intrinsic parameters
#
intrinsic_params = OptionGroup(optp, "Intrinsic Parameters", "Intrinsic parameters (e.g component mass) to use.")
intrinsic_params.add_option("--mass1", type=float, help="Value of first component mass, in solar masses. Required if not providing coinc tables.")
intrinsic_params.add_option("--mass2", type=float, help="Value of second component mass, in solar masses. Required if not providing coinc tables.")
optp.add_option_group(intrinsic_params)

#
# Add the pinnable parameters
#
pinnable = OptionGroup(optp, "Pinnable Parameters", "Specifying these command line options will pin the value of that parameter to the specified value with a probability of unity.")
for pin_param in LIKELIHOOD_PINNABLE_PARAMS:
    option = "--" + pin_param.replace(" ", "-")
    pinnable.add_option(option, type=float, help="Pin the value of %s." % pin_param)
optp.add_option_group(pinnable)

opts, args = optp.parse_args()

# Print command line to a file to recall what was run
cmdname='command.sh'
cmd = open(cmdname, 'w')
cmd.write('#!/usr/bin/env bash\n')
cmd.write(" ".join(sys.argv) )
cmd.close()
st = os.stat(cmdname)
os.chmod(cmdname, st.st_mode | stat.S_IEXEC)

#
# Get trigger information from coinc xml file
#

# Get end time from coinc inspiral table or command line
if opts.coinc_xml is not None:
    xmldoc = utils.load_filename(opts.coinc_xml)
    coinc_table = table.get_table(xmldoc, lsctables.CoincInspiralTable.tableName)
    assert len(coinc_table) == 1
    coinc_row = coinc_table[0]
    event_time = coinc_row.get_end()
    print "Coinc XML loaded, event time: %s" % str(coinc_row.get_end())
elif opts.event_time is not None:
    event_time = glue.lal.LIGOTimeGPS(opts.event_time)
    print "Event time from command line: %s" % str(event_time)
else:
    raise ValueError("Either --coinc-xml or --event-time must be provided to parse event time.")

# get masses from sngl_inspiral_table
if opts.mass1 is not None and opts.mass2 is not None:
    m1, m2 = opts.mass1, opts.mass2
elif opts.coinc_xml is not None:
    sngl_inspiral_table = table.get_table(xmldoc, lsctables.SnglInspiralTable.tableName)
    assert len(sngl_inspiral_table) == len(coinc_row.ifos.split(","))
    m1, m2 = None, None
    for sngl_row in sngl_inspiral_table:
        # NOTE: gstlal is exact match, but other pipelines may not be
        assert m1 is None or (sngl_row.mass1 == m1 and sngl_row.mass2 == m2)
        m1, m2 = sngl_row.mass1, sngl_row.mass2
    event_time = glue.lal.LIGOTimeGPS(opts.event_time)
else:
    raise ValueError("Need either --mass1 --mass2 or --coinc-xml to retrieve masses.")
m1_SI = m1 * lal.LAL_MSUN_SI
m2_SI = m2 * lal.LAL_MSUN_SI
print "Computing marginalized likelihood in a neighborhood about intrinsic parameters mass 1: %f, mass 2 %f" % (m1, m2)

#
# FIXME: Hardcoded values - eventually promote to command line arguments
#
log_dir='logs/' # directory to hold
template_min_freq = 40.
ip_min_freq = 40.
approx = lalsim.TaylorT1
eff_fisher_psd = lal.LIGOIPsd
analyticPSD_Q = True
# The next 4 lines set the maximum size of the region to explore
min_mc_factor = 0.9
max_mc_factor = 1.1
min_eta = 0.05
max_eta = 0.25
# Control evaluation of the effective Fisher grid
NMcs = 11
NEtas = 11
wide_match = 0.90
match_cntr = 0.97 # Fill an ellipsoid of this match
fit_cntr = match_cntr # Do the effective Fisher fit with pts above this match
Nrandpts = opts.N_mass_pts # Requested number of pts to put inside the ellipsoid

#
# Setup signal and IP class
#
param_names = ['Mc', 'eta']
PSIG = lsu.ChooseWaveformParams(
        m1=m1_SI, m2=m2_SI,
        fmin=template_min_freq,
        approx=lalsim.TaylorT1
        )
# Find a deltaF sufficient for entire range to be explored
PTEST = PSIG.copy()
PTEST.m1 *= min_mc_factor
PTEST.m2 *= min_mc_factor
PSIG.deltaF = lsu.findDeltaF(PTEST)

PTMPLT = PSIG.copy()

IP = lsu.Overlap(fLow = ip_min_freq,
        deltaF = PSIG.deltaF,
        psd = eff_fisher_psd,
        analyticPSD_Q = analyticPSD_Q
        )

hfSIG = lsu.norm_hoff(PSIG, IP)
McSIG = lsu.mchirp(m1_SI, m2_SI)
etaSIG = lsu.symRatio(m1_SI, m2_SI)

# Find appropriate parameter ranges
min_mc = McSIG * min_mc_factor
max_mc = McSIG * max_mc_factor
param_ranges = eff.find_effective_Fisher_region(PSIG, IP, wide_match,
        param_names, [[min_mc, max_mc],[min_eta, max_eta]])
print "Computing amibiguity function in the range:"
for i, param in enumerate(param_names):
    if param=='Mc' or param=='m1' or param=='m2': # rescale output by MSUN
        print "\t", param, ":", np.array(param_ranges[i])/lal.LAL_MSUN_SI,\
                "(Msun)"
    else:
        print "\t", param, ":", param_ranges[i]

# setup uniform parameter grid for effective Fisher
pts_per_dim = [NMcs, NEtas]
Mcpts, etapts = eff.make_regular_1d_grids(param_ranges, pts_per_dim)
etapts = map(lsu.sanitize_eta, etapts)
McMESH, etaMESH = eff.multi_dim_meshgrid(Mcpts, etapts)
McFLAT, etaFLAT = eff.multi_dim_flatgrid(Mcpts, etapts)
dMcMESH = McMESH - McSIG
detaMESH = etaMESH - etaSIG
dMcFLAT = McFLAT - McSIG
detaFLAT = etaFLAT - etaSIG
grid = eff.multi_dim_grid(Mcpts, etapts)

# Change units on Mc
dMcFLAT_MSUN = dMcFLAT / lal.LAL_MSUN_SI
dMcMESH_MSUN = dMcMESH / lal.LAL_MSUN_SI
McMESH_MSUN = McMESH / lal.LAL_MSUN_SI
McSIG_MSUN = McSIG / lal.LAL_MSUN_SI

# Evaluate ambiguity function on the grid
rhos = np.array(eff.evaluate_ip_on_grid(hfSIG, PTMPLT, IP, param_names, grid))
rhogrid = rhos.reshape(NMcs, NEtas)

# Fit to determine effective Fisher matrix
cut = rhos > fit_cntr
fitgamma = eff.effectiveFisher(eff.residuals2d, rhos[cut], dMcFLAT_MSUN[cut],
        detaFLAT[cut])
# Find the eigenvalues/vectors of the effective Fisher matrix
gam = eff.array_to_symmetric_matrix(fitgamma)
evals, evecs, rot = eff.eigensystem(gam)

# Print information about the effective Fisher matrix
# and its eigensystem
print "Least squares fit finds g_Mc,Mc = ", fitgamma[0]
print "                        g_Mc,eta = ", fitgamma[1]
print "                        g_eta,eta = ", fitgamma[2]

print "\nFisher matrix:"
print "eigenvalues:", evals
print "eigenvectors:"
print evecs
print "rotation taking eigenvectors into Mc, eta basis:"
print rot

#
# Distribute points inside predicted ellipsoid of certain level of overlap
#
r1 = np.sqrt(2.*(1.-match_cntr)/evals[0]) # ellipse radii along eigendirections
r2 = np.sqrt(2.*(1.-match_cntr)/evals[1])
# Get pts. inside an ellipsoid oriented along eigenvectors...
rand_grid = eff.uniform_random_ellipsoid(Nrandpts, r1, r2)
# Rotate to get coordinates in parameter basis
rand_grid = np.array([ np.real( np.dot(rot, rand_grid[i]))
    for i in xrange(len(rand_grid)) ])
# Put in convenient units,
# change from parameter differential (i.e. dtheta)
# to absolute parameter value (i.e. theta = theta_true + dtheta)
rand_dMcs_MSUN, rand_detas = tuple(np.transpose(rand_grid)) # dMc, deta
rand_Mcs = rand_dMcs_MSUN * lal.LAL_MSUN_SI + McSIG # Mc (kg)
rand_etas = rand_detas + etaSIG # eta

# Prune points with unphysical values of eta from rand_grid
rand_etas = np.array(map(partial(lsu.sanitize_eta, exception=np.NAN), rand_etas))
rand_grid = np.transpose((rand_Mcs,rand_etas))
phys_cut = ~np.isnan(rand_grid).any(1) # cut to remove unphysical pts
rand_grid = rand_grid[phys_cut]
print "Requested", Nrandpts, "points inside the ellipsoid of",\
        match_cntr, "match."
print "Kept", len(rand_grid), "points with physically allowed parameters."

# Convert to m1, m2
m1m2_grid = np.array([lsu.m1m2(rand_grid[i][0], rand_grid[i][1])
        for i in xrange(len(rand_grid))])
m1m2_grid /= lal.LAL_MSUN_SI

from glue.ligolw import utils, ligolw, lsctables, ilwd
from glue.ligolw.utils import process

if opts.mass_points_xml:
    xmldoc = ligolw.Document()
    xmldoc.appendChild(ligolw.LIGO_LW())
    procrow = process.append_process(xmldoc, program=sys.argv[0])
    procid = procrow.process_id
    process.append_process_params(xmldoc, procrow, process.process_params_from_dict(opts.__dict__))
    
    sim_insp_tbl = lsctables.New(lsctables.SimInspiralTable, ["simulation_id", "process_id", "mass1", "mass2"])
    for itr, (m1, m2) in enumerate(m1m2_grid):
        sim_insp = sim_insp_tbl.RowType()
        sim_insp.simulation_id = ilwd.ilwdchar("sim_inspiral:sim_inspiral_id:%d" % itr)
        sim_insp.process_id = procid
        sim_insp.mass1, sim_insp.mass2 = m1, m2
        sim_insp_tbl.append(sim_insp)
    xmldoc.childNodes[0].appendChild(sim_insp_tbl)
    ifos = "".join([o.split("=")[0][0] for o in opts.channel_name])
    start = int(event_time)
    fname = "%s-MASS_POINTS-%d-1.xml.gz" % (ifos, start)
    utils.write_filename(xmldoc, fname, gz=True)

# Write the sub file and DAG
from glue import pipeline
dag = pipeline.CondorDAG(log=os.getcwd())

mkdir(log_dir) # Make a directory to hold log files of jobs

ile_job_type, ile_sub_name = dag_utils.write_integrate_likelihood_extrinsic_sub(
        tag='integrate',
        log_dir=log_dir,
        cache_file=opts.cache_file,
        channel_name=opts.channel_name,
        psd_file=opts.psd_file,
        coinc_xml=opts.coinc_xml,
        reference_freq=opts.reference_freq,
        fmax=opts.fmax,
        event_time=event_time,
        time_marginalization=opts.time_marginalization,
        save_samples=opts.save_samples,
        output_file=opts.output_file,
        n_eff=opts.n_eff,
        n_max=opts.n_max,
        ncopies=opts.n_copies,
        save_deltalnL=opts.save_deltalnL,
        save_P=opts.save_P,
        n_chunk=opts.n_chunk,
        adapt_floor_level=opts.adapt_floor_level,
        adapt_weight_exponent=opts.adapt_weight_exponent
        )
ile_job_type.write_sub_file()

#
# Make the posterior plot here since we need to make it the child of every sql
# node in the DAG
#
pos_plot_job_type, pos_plot_job_name = dag_utils.write_posterior_plot_sub(tag="pos_plot", log_dir=log_dir)
pos_plot_job_type.write_sub_file()
pos_plot_node = pipeline.CondorDAGNode(pos_plot_job_type)
pos_plot_node.set_pre_script(dag_utils.which("coalesce.sh"))
pos_plot_node.set_category("PLOT")
dag.add_node(pos_plot_node)

sql_job_type, sql_job_name = dag_utils.write_result_coalescence_sub(tag="coalesce", log_dir=log_dir)
sql_job_type.write_sub_file()

for i, (m1, m2) in enumerate(m1m2_grid):
    ile_node = pipeline.CondorDAGNode(ile_job_type)
    ile_node.add_macro("macromass1", m1)
    ile_node.add_macro("macromass2", m2)

    mass_grouping = "MASS_SET_%d" % i

    # This is to identify output from groupings of the sane mass point
    ile_node.add_macro("macromassid", mass_grouping)

    ile_node.set_category("ILE")
    dag.add_node(ile_node)

    sql_node = pipeline.CondorDAGNode(sql_job_type)
    sql_node.add_parent(ile_node)

    # The sql node needs to run a PRE script in order to coalesce the data into
    # a cache
    sql_node.set_pre_script(dag_utils.which("coalesce.sh"))
    sql_node.add_pre_script_arg(mass_grouping)

    # This is to identify output from groupings of the sane mass point
    sql_node.add_macro("macromassid", mass_grouping)

    sql_node.set_category("SQL")
    dag.add_node(sql_node)

    tri_plot_job_type, tri_plot_job_name = dag_utils.write_tri_plot_sub(tag="tri_plot", log_dir=log_dir)
    tri_plot_job_type.write_sub_file()
    tri_plot_node = pipeline.CondorDAGNode(tri_plot_job_type)
    tri_plot_node.add_macro("macromassid", mass_grouping)
    tri_plot_node.set_category("PLOT")
    dag.add_node(tri_plot_node)
    tri_plot_node.add_parent(sql_node)

    pos_plot_node.add_parent(tri_plot_node)

dag_name="marginalize_extrinsic_parameters"
dag.set_dag_file(dag_name)
dag.write_concrete_dag()

print "Created a DAG named %s\n" % dag_name
print "This will run %i instances of %s in parallel\n" % (len(rand_grid), ile_sub_name)
