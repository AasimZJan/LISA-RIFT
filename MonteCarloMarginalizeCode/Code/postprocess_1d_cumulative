#! /usr/bin/env python
#
#  - construct 1d cumulative (from flatfile), given argument type specification.
#     - can also construct 2d skymap
#  - evaluate the p value of the injection's location (or constant L contour)
#  - plot the cumulative
#
# USAGE
#    postprocess_1d_cumulative --save-sampler-file flatfile  --inj inj.xml --event 0
#          # input from flatfile-points.xml.gz
#          # output to flatfile-posterior-*
#                 - skymap
#                 - 1d cumulatives in each parameter
#                 - cumulative data at injection prameters
#    postprocess_1d_cumulative --save-sampler-file flatfile  --adapt-beta 0
#          # weight as L^\beta p/ps
#          # using 0 treats all mass points the same.
#          # Note that if only a fraction of the extrinsic points are saved, using \beta=0 produces
#             a conservative skymap
#
# PHYSICS NOTES
#   - *Requires a mass prior implicitly! *
#        : this script joins results from different simulations, hence different masses.
#          Right now, this prior is *not* properly implemented when creating relative weights at different mass points:
#          In practice, we are using a uniform prior in mchirp, eta.  That's not the physical prior. 
#      ===> This needs to be corrected. <===
#
# PORTABILITY NOTES
#    np.histogram :  density argument not supported on LIGO clusters' version

# Setup. 
import numpy as np
import lal
import lalsimutils
import bisect
# Plot always, for now
import matplotlib
# matplotlib.use("Agg")  # Uncomment this line on clusters.  Need a better solution...
fExtension = "jpeg"
if matplotlib.get_backend() == "MacOSX":
    fExtension = "jpeg"
if matplotlib.get_backend() == "agg":
    fExtension  = "png"
print " matplotlib backend ", matplotlib.get_backend(), " so using file type ", fExtension
from matplotlib import pylab as plt
from mpl_toolkits.mplot3d import Axes3D


def mean_and_dev(arr, wt):
    av = np.average(arr, weights=wt)
    var = np.average(arr*arr, weights=wt)
    return [av, var - av*av]

def pcum_at(val,arr, wt):
    nm = np.sum(wt)
    return np.sum(wt[np.where(val < arr)]/nm)


# Parse standard arguments
import ourparams
import ourio
opts, rosDebugMessagesDictionary = ourparams.ParseStandardArguments()

# Import flatfile (default).  Modify in future to take xml
# Should use first line to establish associations!
# For now, assume output in format provided by 'convert_output_format_ile2inference':
# Columns assumed sorted as <stuff>, tref, phi, incl, psi, ra, dec, dist, lnL,p, ps
#  * assume* we have already sorted these by importance?
ret = np.loadtxt(opts.points_file_base+"-points.dat")
lnLmax = np.max(ret[:,-1])  # only relative weights are needed, so avoid loss of precision and infinty
weights = np.exp(opts.adapt_beta*(ret[:,-1] - lnLmax))*ret[:,-3]/ret[:,-2]
# if opts.adapt_beta == 0:
#     weights = np.ones(len(weights))

# Metadata and pp data
# Note mc, eta not always supported, but m1, m2 *are very likely* always be the first two
values = {}
metadata = {}
ppdata = {}
values["ra"] = ret[:,-3-7]
values["dec"] = ret[:,-3-6]
values["tref"] = ret[:,-3-5]
values["phi"] = ret[:,-3-4]
values["incl"] = ret[:,-3-3]
values["psi"] = ret[:,-3-2]
values["dist"] = ret[:,-3-1]
values["mc"] = lalsimutils.mchirp(ret[:,0],ret[:,1])
values["eta"] = lalsimutils.symRatio(ret[:,0],ret[:,1])

for key in values.keys():
    metadata[key] =  mean_and_dev(values[key], weights)

if opts.inj:
    Psig = ourparams.PopulatePrototypeSignal(opts)
    ppdata['ra'] = [Psig.phi,pcum_at(Psig.phi,values["phi"],weights)]
    ppdata['dec'] = [Psig.theta,pcum_at(Psig.theta,values["dec"], weights)]
    ppdata['phi'] = [Psig.phiref,pcum_at(Psig.phiref,values["phi"], weights)]
    ppdata['tref'] = [Psig.tref, 0]  # to make sure this exists for all keys
    ppdata['incl'] = [Psig.incl,pcum_at(Psig.incl, values["incl"], weights)]
    ppdata['psi'] = [Psig.psi,pcum_at(Psig.psi,values["psi"], weights)]
    ppdata['dist'] = [Psig.dist/(1e6*lal.LAL_PC_SI),pcum_at(Psig.dist/(1e6*lal.LAL_PC_SI),values["dist"], weights)]

    # See comment about mass prior, above! 
    m1Sun = Psig.m1/lal.LAL_MSUN_SI
    m2Sun = Psig.m2/lal.LAL_MSUN_SI
    mc = lalsimutils.mchirp(m1Sun,m2Sun)
    eta = lalsimutils.symRatio(m1Sun,m2Sun)
    ppdata['mc'] = [mc, pcum_at(mc, values["mc"], weights)]
    ppdata['eta'] = [eta, pcum_at(mc, values["eta"], weights)]

    with open(opts.points_file_base+"-postprocess-pp.dat",'w') as f:
        for key in ['ra','dec', 'phi', 'incl', 'psi', 'dist']: 
            f.write(key + " " + str(ppdata[key][0]) + ' '+ str(ppdata[key][1]) + '\n')


###
### 1d cumulative plots
###


# Construct 1d plots.  See 'ourio.py'.  Prior code not used since sampler unavailable here to set range limits
nFig = 0
keynames = values.keys() # ["ra", "dec", "tref", "phi","incl", "psi", "dist" ]   # Need  
vals = np.zeros(len(values["phi"]))
wts = np.zeros(len(values["phi"]))
for nFig in np.arange(len(keynames)):
    param = keynames[nFig]
    plt.figure(nFig)
    # Sample distributions.  Copy needed becuase of sorting and sum
    vals = values[param]
    wts = weights

    plt.clf()
    # Stage 2: Cumulative.  *High detail* Note this can be done with 'cumulative=True' in np
    # This also is used to set the plot range, at the 99.9% and 0.1% confidence intervals -- very important for parameters like mc, eta
    idx_sorted_util  = np.lexsort((np.arange(len(vals)), vals))
    vals  = np.array([vals[k] for k in idx_sorted_util])
    wts  = np.array([wts[k] for k in idx_sorted_util])
    cum_wts = np.cumsum(wts)
    cum_wts = cum_wts/cum_wts[-1]
    xlow = vals[bisect.bisect(cum_wts, 0.001)]
    xhigh = vals[bisect.bisect(cum_wts, 0.999)]
    plt.plot(vals,cum_wts,label="posterior:"+param)
    if xlow < xhigh:  # can be a problem with t
        plt.xlim(xlow,xhigh)
    if opts.inj:
        plt.plot([ppdata[param][0],ppdata[param][0]], [1,1], color='k',linestyle='--')
   # hist, bins = plt.hist(vals, bins=100, normed=1, weights=weights,cumulative=True)
    plt.xlabel(param)
    plt.title("1d cumulative for "+param)
    plt.legend()
    plt.savefig(opts.points_file_base+"-posterior-cumulative-"+param+"-1d."+fExtension)


    # Stage 1: PDF
    plt.clf()
#    hist, bins = np.histogram(vals, bins=100, density=True, weights=weights)  # density=True is not portable enough
    hist, bins = np.histogram(vals, bins=100,  weights=weights)
    dx = bins[1]-bins[0]
    hist = hist/(np.sum(hist)*dx)
    center = (bins[:-1]+bins[1:])/2
    plt.plot(center,hist,label="posterior:"+param+":sampled")
#    hist, bins  = np.histogram(vals,bins=50,density=True)
    hist, bins  = np.histogram(vals,bins=50)
    dx = bins[1]-bins[0]
    hist = hist/(np.sum(hist)*dx)
    center = (bins[:-1]+bins[1:])/2
    plt.plot(center,hist,label="saved samples:"+param+"",linestyle='--')
    if opts.inj:
        plt.plot([ppdata[param][0],ppdata[param][0]], [np.max(hist),np.max(hist)], color='k',linestyle='--')
#    plt.xlim(xlow,xhigh)
    plt.xlabel(param)
    plt.title("1d density for "+param)
    plt.legend()
    plt.savefig(opts.points_file_base+"-posterior-density-"+param+"-1d."+fExtension)
    plt.clf()


###
### Sky plots
###
# Add posterior skymap: use a *fixed* resolution.  (Better adaptive solutions in routines by Leo Singer)
import healpy
nside = 32   # must be a power of 2.  This is 12,300 sky points, good enough for moderate-SNR work
npix = healpy.nside2npix(nside)
smap = np.zeros(npix)
print " Sky resolution note: ", len(smap), len(weights)

# Convert to theta, phi from ra, dec
values["theta"] = 0.5*np.pi  - values["dec"]
values["phi"] = values["ra"]

# Convert sky locations to discrete pixels
values["index"] = healpy.ang2pix(nside, values["theta"], values["phi"],nest=False)

# Loop over all pixel values.  Add more to the skymap.  [*slow* loop. Probably better python-y solution]
for i in np.arange(len(weights)):
    smap[values["index"][i]]+= weights[i]
smap = smap/np.sum(smap)

# Write skymap fits file
import lalinference.bayestar.fits
lalinference.bayestar.fits.write_sky_map(opts.points_file_base+"-skymap.fits.gz", smap,
    creator="postprocess_1d_cumulative")

# Create plot
from lalinference.bayestar import fits as bfits
from lalinference.bayestar import plot as bplot
plt.clf()
plt.subplot(111, projection='astro mollweide')
bplot.healpix_heatmap(smap)
plt.savefig(opts.points_file_base+"-posterior-skymap."+fExtension)


###
### Triplots : because sql locks are ubiquitous
###

import bayesutils
p_syms = ["$\\iota$", "d", "$\\delta$", "$\\alpha$", "t", "$\\phi$", "$\\psi$"]
# This causes a lot of memory duplication.  Would be better to use 'ret', above,
samples = (np.array([values["incl"],values["dist"], values["dec"], values["ra"], values["tref"], values["phi"], values["psi"]])).T
print len(samples), samples.shape
if opts.inj:
    injection = np.array([ppdata['incl'][0], ppdata['dist'][0], ppdata['dec'][0], ppdec["ra"][0], ppdec["tref"][0], ppdata["phi"][0], ppdata["psi"][0]])
else:
    injection=None
bayesutils.triplot(samples, weights=weights, labels=p_syms, title=None, inj=injection)
if matplotlib.get_backend() == "MacOSX":
    plt.savefig(opts.points_file_base+ "-triplot.pdf")   # Workaround for Richard's laptop
else:
    plt.savefig(opts.points_file_base+ "-triplot."+fExtension)
